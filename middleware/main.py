"""
This module implements a FastAPI server that provides endpoints for interacting with the DocumentAgent.
The server exposes three endpoints:

1. /ask (POST):
   - Receives a transcription (user's spoken text) and processes it synchronously through the DocumentAgent.
   - Returns the generated answer as a JSON response.

2. /transcribe (POST):
   - Receives a transcription from the Furhat frontend.
   - Processes the transcription asynchronously by passing it to the DocumentAgent.
   - Stores the agent's response for later retrieval.
   - Returns an acknowledgment that the transcription was received.

3. /response (GET):
   - Retrieves the latest response generated by the DocumentAgent.
   - Returns the response in a JSON format so that the Furhat frontend can vocalize it.

The server integrates with a backend agent (DocumentAgent) which is responsible for managing
the conversation flow, retrieving relevant document context, and generating responses using an LLM.
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn

# Import the DocumentAgent from the backend folder.
from my_furhat_backend.agents.document_agent import DocumentAgent

app = FastAPI()

# Define a Pydantic model for incoming transcription.
class Transcription(BaseModel):
    content: str

# Global variable to store the latest agent response.
latest_response = None

# Instantiate the agent.
agent = DocumentAgent()

@app.post("/ask")
async def ask_question(transcription: Transcription):
    """
    Process a transcription by running it through the LLM agent and returning the generated response.

    This endpoint receives a POST request containing a transcription (user's spoken text),
    passes the transcription to the DocumentAgent's `run` method to generate an answer, and returns
    the result in a JSON response. If an error occurs during processing, a 500 HTTPException is raised.

    Parameters:
        transcription (Transcription): A Pydantic model instance containing the 'content' field
                                         with the transcribed text from the Furhat frontend.

    Returns:
        dict: A JSON response with a single key 'response' that holds the generated answer as a string.
    """
    try:
        # Run the agent with the given transcription content.
        response_text = agent.run(transcription.content)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    return {"response": response_text}


@app.post("/transcribe")
async def transcribe(transcription: Transcription):
    """
    Receive transcribed text from the Furhat frontend and process it with the LLM agent.

    This endpoint accepts a POST request containing the transcription (user's spoken text),
    processes it using the DocumentAgent's `run` method, and stores the generated response
    in a global variable for later retrieval via the /response endpoint.
    If an error occurs during processing, a 500 HTTPException is raised.

    Parameters:
        transcription (Transcription): A Pydantic model instance with the 'content' field containing
                                         the transcribed text.

    Returns:
        dict: A JSON response indicating that the transcription was received.
    """
    global latest_response
    try:
        # Process the transcription with the agent.
        latest_response = agent.run(transcription.content)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    return {"status": "transcription received"}


@app.get("/response")
async def get_response():
    """
    Retrieve the latest response generated by the LLM agent.

    This endpoint accepts a GET request and returns the most recent response stored from a transcription.
    If no response is available yet, it returns a message indicating that no response has been generated.

    Returns:
        dict: A JSON response with a 'response' key that contains the agent's answer as a string.
    """
    if latest_response is None:
        return {"response": "No response generated yet."}
    return {"response": latest_response}


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
